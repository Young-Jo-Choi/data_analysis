{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_huber(threshold=1.0):\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = threshold*tf.abs(error) - (threshold**2)*0.5\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사용자 정의 모델과 훈련 알고리즘\n",
    "## 활성화 함수, 초기화, 규제, 제한을 커스터마이징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softplus(z): # == tf.nn.softplus(z), keras.activations.softplus()\n",
    "    return tf.math.log(tf.exp(z)+1.0)\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2./ shape[0]+shape[1])\n",
    "    return tf.random.normal(shape, stddev = stddev, dtype=dtype)\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01*weights))\n",
    "def my_positive_weigth(weights): ## relu\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(30, activation=my_softplus,\n",
    "                          kernel_constraint=my_positive_weigth,\n",
    "                          kernel_initializer=my_glorot_initializer,\n",
    "                          kernel_regularizer=my_l1_regularizer)\n",
    "\n",
    "# factor 하이퍼파라미터를 저장하는 l1 규제를 위한 클래스\n",
    "class MyL1Regularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self,factor):\n",
    "        self.factor = factor\n",
    "    # ()를 붙여 실행시 동작하는 메서드\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor*weights))\n",
    "    def get_config(self):\n",
    "        return {'factor':self.factor}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(모델을 저장할 때 keras는 손실 객체의 get_config() 메서드를 호출한다.)\n",
    "\n",
    "## 사용자 정의 지표\n",
    "mse, accuracy 등을 말한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미리 정의한 huber loss를 지표로 사용할 수도 있다.\n",
    "model.compile(loss='mse',optimizer='nadam',metrics[create_huber(2.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트리밍 지표 만들기\n",
    "# (배치마다 점진적으로 업데이트되기 때문에 스트리밍 지표라고 부른다.)\n",
    "class HuberMetric(keras.metrics.Metric):\n",
    "    def __init__(self,threshold=1.0,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        self.total = self.add_weight('total',initializer='zeros')\n",
    "        self.count = self.add_weight('count',initializer='zeros')\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(metric))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true),tf.float32))\n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, 'threshold':self.threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용자 정의 층\n",
    "### 가중치가 없는 층(ex) Flatten, ReLU 등)\n",
    "python 함수를 만든 뒤 keras.layers.Lambda로 감싸주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "exponential_layer = keras.layers.Lambda(lambda x:tf.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가중치가 있는 층(상태가 있는 층)\n",
    "keras.layers.Layer를 상속해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense 층의 간소화 버전을 구현\n",
    "# 다른 층과 동일하게 사용가능\n",
    "\n",
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "    # 가중치마다 add_weight 변수를 호출해 층의 변수를 만든다.\n",
    "    # backward와 관련된 부분은 상속을 통해 구현되어있을 것\n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            name = 'kernel',shape = [batch_input_shape[-1], self.units],\n",
    "            initializer='glorot_normal')\n",
    "        self.bias = self.add_weight(\n",
    "            name = 'bias',shape = [self.units], initializer='zeros')\n",
    "        # 마지막에 호출해야\n",
    "        super().build(batch_input_shape)\n",
    "    # 해당 층에 필요한 연산\n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "    # 해당 층의 출력 크기 반환\n",
    "    # ex) 300*20 -> Dense(10) -> 300*10\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.Tensorshape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"units\":self.units,\n",
    "               \"activation\":keras.activations.serialize(self.activation)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러 출력을 가진 층을 만드려며뉴 call() 메서드가 출력의 리스트를 반환해야하고 <br>\n",
    "compute_output_shape()는 배치 출력의 크기의 리스트를 반환해야한다.<br>\n",
    "다음 예시는 두 개의 입력과 세 개의 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMultiLayer(keras.layers.Layer):\n",
    "    def call(self, X):\n",
    "        X1, X2 = X\n",
    "        return [X1+X2, X1*X2, X1/X2]\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        b1,b2 = batch_input_shape\n",
    "        return [b1,b1,b1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만일 훈련과 테스트에서 다르게 동작하는 층이 필요하다면 call() 메서드에 training 매개변수를 추가해 훈련인지 테스트인지 결정하면 된다.<br>\n",
    "(ex) dropout, batch normalization)<br>\n",
    "다음 예시는 훈련 중에는 규제를 위해 가우스 잡음을 추가하고 테스트 시에는 아무것도 하지 않는 층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGaussianNoise(keras.layers.Layer):\n",
    "    def __init__(self,stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "    def call(self, X, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X),stddev=self.stddev)\n",
    "            return X + noise\n",
    "        else:\n",
    "            return X\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용자 정의 모델\n",
    "1. keras.Model 클래스 상속\n",
    "2. 생성자에서 층과 변수를 만듦\n",
    "3. 모델이 해야할 작업을 call() 메서드에 구현\n",
    "\n",
    "다음 예시는 input -> Dense1 -> ResidualBlock1 (ResidualBlock1만 *3번 반복) -> ResidualBlock2 -> Dense2 ->...<br>\n",
    "ResidualBlock : input -> Dense_1 -> Dense_2 -> (input + Dense_2의 output)\n",
    "\n",
    "keras.models.load_model()을 이용해 저장된 모델을 로드하고 싶다면 두 클래스 모두에 get_config() 메서드 구현해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(keras.layers.Layer):\n",
    "    def __init__(self, n_layer, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(n_neurons, activation='elu',\n",
    "                                         kernel_initializer='he_normal') for _ in layer(n_layers)]\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z\n",
    "\n",
    "class ResidualRegressor(keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(30, activation='elu',\n",
    "                                         kernel_initializer='he_normal')\n",
    "        self.block1 = ResidualBlock(2,30)\n",
    "        self.block2 = ResidualBlock(2,30)\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        for _ in range(1+3):\n",
    "            Z = self.block1(Z)\n",
    "        Z = self.block2(Z)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 구성 요소에 기반한 손실과 지표\n",
    "다음 모델은 맨 위의 은닉층에 보조 출력을 갖는다. 이 보조 출력에 연결된 손실을 $\\textbf{재구성 손실}$이라고 부른다.(=재구성과 입력 사이의 mse)<br>\n",
    "이 재구성 손실은 주 손실에 더해 회귀 작업에 직접적인 도움은 되지 않더라도 모델이 은닉층을 통과하며 가능한 많은 정보를 유지하도록 유도한다. 이런 손실이 이따금 일반화 성능을 향상시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructingRegressor(keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(30, activation='selu',\n",
    "                                         kernel_initializer='lecun_normal') for _ in range(5)]\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "        \n",
    "    # 완전 연결층을 하나 더 추가하여 모델의 입력을 재구성하는데 사용\n",
    "    # 완전연결층의 유닛 개수는 입력 개수와 같아야한다.\n",
    "    # 재구성 층을 굳이 build() 메서드에서 만드는 이뉴는 이 메서드가 호출되기 전까지 입력 개수를 알 수 없기 때문\n",
    "    def build(self, batch_input_shape):\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = keras.layers.Dense(n_inputs)\n",
    "        super().build(batch_input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        # 모델의 손실 리스트에 추가(단 재구성 손실이 주 손실을 압도하지 않도록 0.005를 곱해 크기를 줄임)\n",
    "        self.add_loss(0.05*recon_loss)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자동 미분을 사용하여 그래디언트 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w1,w2):\n",
    "    return (3*w1**2) + (2*w1*w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.000003007075065\n",
      "10.000000003174137\n"
     ]
    }
   ],
   "source": [
    "w1, w2 = 5, 3\n",
    "eps = 1e-6\n",
    "print((f(w1+eps,w2)-f(w1,w2))/eps)\n",
    "print((f(w1,w2+eps)-f(w1,w2))/eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "근사값은 약간의 오차를 포함하는데다가 매 파라미터마다 적어도 한번씩 f()가 호출되므로 대규모 신경망에서는 적용하기 어렵기 때문에 자동미분을 사용해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1,w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "\n",
    "# 메모리를 아끼기 위해서는 다음 블럭에 최소한만 담아야한다.\n",
    "# 블록 안에서 with tape.stop_recording() 블록을 만들어 계산을 기록하지 않을 수 있음\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1,w2)\n",
    "gradients = tape.gradient(z, [w1,w2])\n",
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(36.0, shape=(), dtype=float32)\n",
      "tf.Tensor(10.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 해당 인자가 없으면 gradient 메서드를 한번 밖에 호출하지 못함\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = f(w1,w2)\n",
    "    \n",
    "dz_dw1 = tape.gradient(z, w1)\n",
    "dz_dw2 = tape.gradient(z, w2)\n",
    "print(dz_dw1)\n",
    "print(dz_dw2)\n",
    "\n",
    "del tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.Variable이 아닌 다른 객체(ex)tf.constant)에 대한 그래디언트를 계산하면 None이 반환됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모든 tensor를 감시해 연산을 기록할 수 있음\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(w1)\n",
    "    tape.watch(w2)\n",
    "    z = f(w1,w2)\n",
    "    \n",
    "gradients = tape.gradient(z, [w1,w2])\n",
    "gradients    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같은 기법은 입력이 작을 때 변동 폭이 큰 활성화 함수에 대한 규제 손실을 구현하는 경우 유용함<br>\n",
    "입력은 변수가 아니므로 테이프에 기록을 명시적으로 알려주어야 한다.\n",
    "\n",
    "(자세한 것은 hands on machine learning - github -'자동 미분으로 그래디언트 계산하기'를 참조)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=30.0>, None]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 신경망 일부에 그래디언트의 역전파를 막을 수도 있음\n",
    "def f(w1,w2):\n",
    "    return (3*w1**2) + tf.stop_gradient(2*w1*w2)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1,w2)\n",
    "\n",
    "gradients = tape.gradient(z,[w1,w2])\n",
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1,), dtype=float32, numpy=array([nan], dtype=float32)>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 자동미분으로 다음 함수의 그래디언트를 계산하는 것이 수치적으로 불안정하기 때문에 NaN이 반환(부동소수점 오류로 무한 나누기 무한 계산)\n",
    "x = tf.Variable([100.])\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_softplus(x)\n",
    "    \n",
    "tape.gradient(z,[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 수치적으로 안전한 softplus의 도함수(1/(1+1/exp(x)))를 해석적으로 구하고, 다음 데코레이터를 사용하고, \n",
    "# 일반 출력과 도함수를 계산하는 함수를 반환하여 안전하게 그래디언트를 계산하는 함수를 만들 수 있음\n",
    "@tf.custom_gradient\n",
    "def my_better_softplus(z):\n",
    "    exp = tf.exp(z)\n",
    "    def my_softplus_gradients(grad):\n",
    "        return grad/(1+1/exp)\n",
    "    return tf.math.log(exp+1), my_softplus_gradients\n",
    "# 여전히 지수함수이기 때문에 폭주할 수 있는데, tf.where을 사용해 값이 클 때 입력을 그대로 반환하는 방법도 있음\n",
    "\n",
    "x = tf.Variable([100.])\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_better_softplus(x)\n",
    "    \n",
    "tape.gradient(z,[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용자 정의 훈련 반복\n",
    "두개의 다른 옵티마이저를 사용하고 싶은데 .fit() 메서드는 하나의 옵티마이저만 사용, 해당 기능을 위해 훈련 반복을 직접 구현<br>\n",
    "혹은 의도한 대로 잘 동작하는지 확신을 갖기 위해 사용자 정의 훈련 반복을 사용할 수도 있다.\n",
    "\n",
    "(다만 사용자 정의 훈련은 버그가 발생하기 쉽고, 유지 보수하기 어려운 코드가 되므로 극도의 유연성이 필요한 경우가 아니라면 fit() 메서드 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "df['target'] = boston.target\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "scaler = StandardScaler()\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,:-1], df.iloc[:,-1])\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_train_scaled = X_train_scaled.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "l2_reg = keras.regularizers.l2(0.05)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu',kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=l2_reg),\n",
    "    keras.layers.Dense(1,kernel_regularizer=l2_reg)\n",
    "])\n",
    "\n",
    "# 훈련세트에서 샘플 배치를 랜덤하게 추출하는 함수\n",
    "def random_batch(X,y,batch_size=32):\n",
    "    idx = np.random.randint(len(X),size=batch_size)\n",
    "    return X[idx], y[idx]\n",
    "# 현재 스텝 수, 전체 스텝 횟수, 에포크 시작부터 mse 등을 출력하는 함수 생성\n",
    "def print_status_bar(iteration, total, loss, metrics=None):\n",
    "    metrics = '-'.join(['{}: {:.4f}'.format(m.name, m.result())\n",
    "                       for m in [loss]+(metrics or [])])\n",
    "    end = \"\" if iteration < total else '\\n'\n",
    "    print('\\r{}/{} - '.format(iteration,total)+metrics,end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 적용 (뭔가 잘 안되니까 생략)\n",
    "n_epochs = 5\n",
    "batch_size = 4\n",
    "n_steps = len(X_train)// batch_size\n",
    "optimizer = keras.optimizers.Nadam(lr=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.MeanAbsoluteError()]\n",
    "\n",
    "for epoch in range(1,n_epochs+1):\n",
    "    print(f\"epoch {epoch}/{n_epochs}\")\n",
    "    for step in range(1, n_steps+1):\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch, training=True)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss]+model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "        print_status_bar(step * batch_size, len(y_train),mean_loss, metrics)\n",
    "    print_status_bar(len(y_train),len(y_train),mean_loss,metrics)\n",
    "    for metric in [mean_loss]+ metrics:\n",
    "        metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서플로우의 함수와 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cube(x):\n",
    "    return x**3\n",
    "cube(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 텐서플로우 함수(데코레이터로 더 많이 사용)\n",
    "tf_cube = tf.function(cube)\n",
    "\n",
    "# 동일한 결과 반환\n",
    "tf_cube(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.function()은 cube() 함수에서 수행되는 계산을 분석하고 동일한 작업을 수행하는 계산 그래프 생성<br>\n",
    "복잡한 연산을 수행시 파이썬 함수를 빠르게 실행하려면 텐서플로 함수로 바꾸는 것이 좋다.\n",
    "\n",
    "다만 사용자 정의 손실, 지표, 층 등 사용자 정의 함수 작성 후 이를 keras 모델에 사용할 때 케라스는 이 함수를 텐서플로 함수로 변환하므로 tf.constant()를 사용할 필요가 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원본 파이썬 함수처럼 사용\n",
    "tf_cube.python_function(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오토그래프와 트레이싱\n",
    "오토그래프 : 텐서플로우가 파이썬 소스를 분석해 while, if, break, return 등의 요소를 찾음<br>\n",
    "--> 파이썬 함수와 제어문을 텐서플로우의 연산으로 바꾼 업데이트된 버전을 만듦(ex) 반복문 -> tf.while_loop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 (실행하지 마시오)\n",
    "@tf.function\n",
    "def sum_squares(n):\n",
    "    s = 0\n",
    "    for i in tf.range(n+1):\n",
    "        s += i ** 2\n",
    "    return s\n",
    "\n",
    "# --> 오토그래프\n",
    "def tf__sum_squares(n):\n",
    "    s = 0\n",
    "    def loop_body(i,s):\n",
    "        s += i**2\n",
    "        return s,\n",
    "    # 축약\n",
    "    s, = ag__.for_stmt(...,loop_body,(s,))\n",
    "    return s\n",
    "\n",
    "# --> 트레이싱\n",
    "# 노드(연산)와 화살표(텐서) 연결된 형태의 그래프로 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.autograph.to_code(sum_squares.python_function)을 호출하면 생성된 함수의 소스코드를 볼 수 있다. <br>\n",
    "깔끔한 코드는 아니지만 디버깅에 도움이 될 수 있음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
