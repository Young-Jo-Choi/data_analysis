{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 토큰화\n",
        "\n",
        "예시 : 어제 카페 갔었어\n",
        "- 단어 단위 -> 어제,카페,갔었어 / 어제,카페,갔었,어\n",
        "- 문자 단위  -> 어,제,카,페,갔,었,어\n",
        "- 서브워드 단위 : ex) Byte Pair Encoding(BPE), wordpiece\n",
        "\n",
        "## BPE 어휘집합\n",
        ": pre-tokenize 후에 초기 어휘 집합에서 토큰을 2개 씩 (bigram 쌍) 묶어 나열 -> 높은 빈도는 어휘집합에 추가 -> 미리 정한 어휘 집합의 크기를 달성한다면 BPE 어휘집합 구축 절차를 마침<br>\n",
        "=> vocab.json으로 어휘집합을 저장, merge.txt로 병합이력을 모아둠\n",
        "\n",
        "## BPE 토큰화\n",
        "ex) 문장 : pug bug mug<br>\n",
        "병합이력에 u g가 높은 우선 순위로 존재한다 가정 -> p,ug,b,ug,m,ug<br>\n",
        "-> m이 어휘집합에 존재하지 않는다 가정 -> p,ug,b,ug,\\<unk>,ug\n",
        "\n",
        "=> 이런 경우 어휘 잡합의 크기를 합리적으로 유지하면서도 어휘를 구축할 때 보지 못했던 단어(신조어 등)에 대해 유의미한 분절을 수행할 수 있음\n",
        "\n",
        "## wordpiece\n",
        ": 말뭉치에서 자주 등장한 문자열을 토큰으로 인식한다는 점에서 BPE와 유사하지만 문자열 병합의 기준이 '빈도'가 아닌 '우도'<br>\n",
        "병합 후보가 a,b일 때 판단의 근거가 되는 값을 계산하는 방법 = ${\\#ab/n}\\over{(\\#a/n)*(\\#b/n)}$<br>\n",
        "\\#은 각 문자열의 빈도수, n은 전체 글자 수, 즉 분자는 ab가 연이어 등장할 확률, 분모는 a,b가 각각 등장할 확률의 곱\n",
        "\n",
        "wordpiece는 토큰화도 BPE와 약간 다르다. BPE가 merges.txt와 vocal.json을 사용하는데 반해 wordpiece는 vocab.txt만 가지고 토큰화를 진행 <br>\n",
        "-> 분석 대상 어절에 어휘 집합에 있는 서브워드가 포함돼 있을 경우에만 해당 서브워드를 어절에서 분리한다. 단 이러한 서브워드가 여럿 있을 경우 가장 긴 서브워드를 선택한다. <br>\n",
        "-> 어절의 나머지 어휘 집합에 있는 서브워드를 다시 찾고(최장 일치 기준) 또 분리한다. <br>\n",
        "(분석 대상 문자열에서 서브워드 후보가 하나도 없으면 해당 문자열 전체를 미등록 단어로 취급한다.)\n"
      ],
      "metadata": {
        "id": "88Gh0QaWYTrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ratsnlp"
      ],
      "metadata": {
        "id": "f88ZWjSJhGyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "metadata": {
        "id": "i_8ngVXBYSaD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a281a8d9-1c1b-49f3-bd43-31175f48cc1d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EEziWsdNUyln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9295386f-fd8b-4c74-c507-24981f146311"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : e9t@github\n",
            "    Repository : https://github.com/e9t/nsmc\n",
            "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
            "\n",
            "    Naver sentiment movie corpus v1.0\n",
            "    This is a movie review dataset in the Korean language.\n",
            "    Reviews were scraped from Naver Movies.\n",
            "\n",
            "    The dataset construction is based on the method noted in\n",
            "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
            "\n",
            "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nsmc] download ratings_train.txt: 14.6MB [00:00, 29.0MB/s]                            \n",
            "[nsmc] download ratings_test.txt: 4.90MB [00:00, 12.4MB/s]                            \n"
          ]
        }
      ],
      "source": [
        "# 네이버 영화 리뷰 NSMC\n",
        "from Korpora import Korpora\n",
        "nsmc = Korpora.load('nsmc', force_download=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for path, lines in zip(['/content/train.txt', '/content/test.txt'],\n",
        "                       [nsmc.train.get_all_texts(), nsmc.test.get_all_texts()]):\n",
        "    with open(path, 'w',encoding='utf-8') as f:\n",
        "        for line in lines:\n",
        "            f.write(f'{line}\\n')\n"
      ],
      "metadata": {
        "id": "7jwRV9zuhtXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "fJylGyMKsVju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT 토크나이저(BPE) 구축\n",
        "- 단 문자 단위가 아니라 유니코드 바이트 수준으로 어휘 집합을 구축하고 토큰화 진행 (미등록 토큰 문제에서 비교적 자유로움)\n",
        "- 유니코드(UTF-8) : 1바이트를 10진수로 표현하면 0~255의 256개 정수가 되는데 이 정수 각각을 특정 문자로 매핑한 것임\n",
        "\n",
        "어휘 집합 구축 대상 말뭉치를 바이트 단위로 변환하고 이들을 문자 취급해 가장 자주 등장한 문자열을 병합하는 방식으로 어휘 집합을 만드는 것임, 토큰화 역시 문자열을 바이트 단위로 변환한 뒤 수행한다."
      ],
      "metadata": {
        "id": "MQVaIyFGjqWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('/gdrive/MyDrive/Colab Notebooks/Do it 자연어처리/bbpe', exist_ok=True)"
      ],
      "metadata": {
        "id": "QMzejBr9iy1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "bytebpe_tokenizer = ByteLevelBPETokenizer()\n",
        "bytebpe_tokenizer.train(files=['/content/train.txt','/content/test.txt'],\n",
        "                        vocab_size=10000,                   # 어휘 집합 크기 조절\n",
        "                        special_tokens=['[PAD]'])           # 특수 토큰 추가\n",
        "bytebpe_tokenizer.save_model('/gdrive/MyDrive/Colab Notebooks/Do it 자연어처리/bbpe')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFu7HmqYmb4W",
        "outputId": "70764003-6262-4639-c98c-68e97f24ab4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/gdrive/MyDrive/Colab Notebooks/Do it 자연어처리/bbpe/vocab.json',\n",
              " '/gdrive/MyDrive/Colab Notebooks/Do it 자연어처리/bbpe/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bert 토크나이저\n",
        "Bert는 워드피스 토크나이저를 사용"
      ],
      "metadata": {
        "id": "hW6VbTSPoo6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('/gdrive/MyDrive/Colab Notebooks/Do it 자연어처리/wordpiece', exist_ok=True)"
      ],
      "metadata": {
        "id": "59uYkiuZnysW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "wordpiece_tokenizer = BertWordPieceTokenizer(lowercase= False)\n",
        "wordpiece_tokenizer.train(files=['/content/train.txt','/content/test.txt'],\n",
        "                        vocab_size=10000)                   # 어휘 집합 크기 조절\n",
        "wordpiece_tokenizer.save_model('/gdrive/MyDrive/Colab Notebooks/Do it 자연어처리/wordpiece')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMXhESJoo-5H",
        "outputId": "76d6c1bd-8416-4615-9237-e3a67feee64a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/gdrive/MyDrive/Colab Notebooks/Do it 자연어처리/bbpe/vocab.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT 입력값 만들기"
      ],
      "metadata": {
        "id": "tlt_uxcIre6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT 입력값 만들기 : BPE 어휘 집합(vocab.json)과 바이그램 쌍의 병합 우선순위(merge.txt)\n",
        "\n",
        "from transformers import GPT2Tokenizer\n",
        "tokenizer_gpt = GPT2Tokenizer.from_pretrained('/gdrive/MyDrive/Colab Notebooks/Do it 자연어처리/bbpe')\n",
        "tokenizer_gpt.pad_token = \"[PAD]\""
      ],
      "metadata": {
        "id": "eOCrqYj9rgmA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시\n",
        "\n",
        "sentences = ['아 더빙..진짜 짜증나네요 목소리',\n",
        "             '흠...포스터보고 초딩영화줄...오버연기조차 가볍지 않구나',\n",
        "             '별루 였다..']\n",
        "tokenized_sentences = [tokenizer_gpt.tokenize(sentence) for sentence in sentences]\n",
        "print(sentences[0])\n",
        "print(tokenized_sentences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5YPTZ2jWsRX",
        "outputId": "0e7be3f1-e533-47ed-d058-071f2431c5b8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "아 더빙..진짜 짜증나네요 목소리\n",
            "['ìķĦ', 'ĠëįĶë¹Ļ', '..', 'ì§Ħì§ľ', 'Ġì§ľì¦ĿëĤĺ', 'ëĦ¤ìļĶ', 'Ġëª©ìĨĮë¦¬']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 실제 GPT 모델 입력은 다음과 같이 만든다\n",
        "batch_inputs = tokenizer_gpt(\n",
        "    sentences,\n",
        "    padding='max_length',           # 문장 최대 길이에 맞춰 패딩\n",
        "    max_length=12,                  # 문장의 토큰 기준 최대 길이\n",
        "    truncation=True                 # 문장 잘림 허용 옵션\n",
        ")\n"
      ],
      "metadata": {
        "id": "HihBdXc4XLP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "결과로는 input_ids, attention_mask가 만들어짐\n",
        "- input_ids : 토큰화 결과를 가지고 각 토근을 인덱스로 바꾼 것임\n",
        "- 어휘 집합(vocab.json)을 확인해 보면 각 어휘 순서대로 나열된 것을 확인할 수 있는데, 이 순서가 바로 인덱스임"
      ],
      "metadata": {
        "id": "4diWLg9GXgHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰1~토큰12까지의 대한 인덱스를 출력\n",
        "# max_length에 따라 12로 길이가 맞춰짐\n",
        "# 이보다 짧은 문장1과 문장3은 뒤에 [PAD] 토큰에 해당하는 인덱스0이 붙었음 (일종의 dummy 토큰으로 길이를 맞춰주는 역할을 함)\n",
        "# 문장 2는 원래 토큰 길이가 15였는데 truncate=True 옵션으로 잘림\n",
        "print('원래 길이 : ',len(tokenized_sentences[0]),',\\t토큰에 대한 인덱스 : ', batch_inputs['input_ids'][0])\n",
        "print('원래 길이 : ',len(tokenized_sentences[1]),',\\t토큰에 대한 인덱스 : ', batch_inputs['input_ids'][1])\n",
        "print('원래 길이 : ',len(tokenized_sentences[2]),',\\t토큰에 대한 인덱스 : ', batch_inputs['input_ids'][2])"
      ],
      "metadata": {
        "id": "bFhAwTpi0O5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9155b358-63af-4616-f261-6dca29613cdc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원래 길이 :  7 ,\t토큰에 대한 인덱스 :  [334, 2338, 263, 663, 4055, 464, 3808, 0, 0, 0, 0, 0]\n",
            "원래 길이 :  15 ,\t토큰에 대한 인덱스 :  [3693, 336, 2876, 758, 2883, 356, 806, 336, 9875, 875, 2960, 7292]\n",
            "원래 길이 :  4 ,\t토큰에 대한 인덱스 :  [4957, 451, 3653, 263, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- attention_mask는 일반 토큰이 자리한 곳 (1)과 패딩 토큰이 자리한 곳 (0)을 구분해 알려줌"
      ],
      "metadata": {
        "id": "rg_5BB1DYxM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(batch_inputs['attention_mask'][0])\n",
        "print(batch_inputs['attention_mask'][1])\n",
        "print(batch_inputs['attention_mask'][2])"
      ],
      "metadata": {
        "id": "a98_GXT50Oxo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b68d1bc7-6d37-4a19-ea93-4841bfa0984d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bert 입력값 만들기"
      ],
      "metadata": {
        "id": "QAwT_iF4ZEua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer_bert = BertTokenizer.from_pretrained('/gdrive/MyDrive/Colab Notebooks/Do it 자연어처리/wordpiece',\n",
        "                                               do_lower_case=False)\n",
        "tokenized_sentences = [tokenizer_bert.tokenize(sentence) for sentence in sentences]\n",
        "print(sentences[0])\n",
        "print(tokenized_sentences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8Vfu3hGZo_I",
        "outputId": "c6e2ee5f-6150-44db-8ecb-0d4dd984a4b6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "아 더빙..진짜 짜증나네요 목소리\n",
            "['아', '더빙', '.', '.', '진짜', '짜증나', '##네요', '목소리']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 실제 Bert 모델 입력은 다음과 같이 만듦\n",
        "batch_inputs = tokenizer_bert(\n",
        "    sentences,\n",
        "    padding='max_length',           # 문장 최대 길이에 맞춰 패딩\n",
        "    max_length=12,                  # 문장의 토큰 기준 최대 길이\n",
        "    truncation=True                 # 문장 잘림 허용 옵션\n",
        ")"
      ],
      "metadata": {
        "id": "SAEevF65fF7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰의 2와 3인 인덱스는 <CLS>, <SEP>를 가리킴\n",
        "print('원래 길이 : ',len(tokenized_sentences[0]),',\\t토큰에 대한 인덱스 : ', batch_inputs['input_ids'][0])\n",
        "print('원래 길이 : ',len(tokenized_sentences[1]),',\\t토큰에 대한 인덱스 : ', batch_inputs['input_ids'][1])\n",
        "print('원래 길이 : ',len(tokenized_sentences[2]),',\\t토큰에 대한 인덱스 : ', batch_inputs['input_ids'][2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5qV-e-dexfJ",
        "outputId": "912c7941-a43d-4ce9-fc50-947e3433339b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원래 길이 :  8 ,\t토큰에 대한 인덱스 :  [2, 621, 2631, 16, 16, 1993, 3678, 1990, 3323, 3, 0, 0]\n",
            "원래 길이 :  19 ,\t토큰에 대한 인덱스 :  [2, 997, 16, 16, 16, 2609, 2045, 2796, 1981, 1051, 16, 3]\n",
            "원래 길이 :  4 ,\t토큰에 대한 인덱스 :  [2, 3274, 9508, 16, 16, 3, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(batch_inputs['attention_mask'][0])\n",
        "print(batch_inputs['attention_mask'][1])\n",
        "print(batch_inputs['attention_mask'][2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJvWbzvMfSNj",
        "outputId": "6901c90d-a7a9-48fc-e3d2-692792c92ea2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# segment에 해당하는 것으로 모두 0임\n",
        "# Bert 모델은 기본적으로 문서(혹은 문장) 2개를 입력받는데, 둘은 token_type_ids로 구분함\n",
        "batch_inputs['token_type_ids']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1jL4RN7fkIA",
        "outputId": "7c6259ac-8d8f-4142-b7b9-fe7488b38350"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ]
}